# The Llama Cloud API key.
# LLAMA_CLOUD_API_KEY=

# The provider for the AI models to use.
MODEL_PROVIDER=vllm
VLLM_URL=http://10.67.127.42:9333/v1

# The name of LLM model to use.
MODEL=LLM-Research/Meta-Llama-3.1-8B-Instruct

# The provider for the embedding models to use when using vllm as Model Provider. fastembed or ollama (default)
EMBEDDING_PROVIDER=fastembed
EMBEDDING_URL=

# Name of the embedding model to use.
EMBEDDING_MODEL=bge-base-en-v1.5

# Dimension of the embedding model to use.
EMBEDDING_DIM=1024

# The questions to help users get started (multi-line).
# CONVERSATION_STARTERS=

# The OpenAI API key to use.
# OPENAI_API_KEY=

# Temperature for sampling from the model.
# LLM_TEMPERATURE=

# Maximum number of tokens to generate.
LLM_MAX_TOKENS= 1024

# The number of similar embeddings to return when retrieving documents.
TOP_K=5

# The time in milliseconds to wait for the stream to return a response.
STREAM_TIMEOUT=60000

# FILESERVER_URL_PREFIX is the URL prefix of the server storing the images generated by the interpreter.
FILESERVER_URL_PREFIX=http://10.67.127.42:8000/api/files

# The address to start the backend app.
APP_HOST=0.0.0.0

# The port to start the backend app.
APP_PORT=8000

# The system prompt for the AI model.
SYSTEM_PROMPT=WSF is abbreviation of Workload Services Framework. You are a WSF expert and a helpful assistant who helps users with their WSF usage questionse based on provided information.

